{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "World Development Indicators - worldbank.org <br/>\nFabio Cardoso - Dec/2021 <br/><br/>\n\nDatasets: <br/><br/>\n\nCountry Indicators: <br/>\nhttps://datacatalog.worldbank.org/search/dataset/0037712/World-Development-Indicators <br/>\nNote: dataset updated with the topics presented in https://data.worldbank.org/indicator?tab=all - Utils notebook. <br/><br/>\n\nCountry Codes ISO 3166: <br/>\nhttps://www.iban.com/country-codes <br/><br/>\n\nNote: <br/>\nFor some orphans indicators, the topic 'private-sector' was assumed - Utils notebook. <br/>\n. Firms visited or required meetings with tax officials (% of firms) <br/>\n. Net ODA provided to the least developed countries (% of GNI) <br/>\n. Net ODA provided, to the least developed countries (current US )\u2212\ud835\udc41\ud835\udc52\ud835\udc61\ud835\udc42\ud835\udc37\ud835\udc34\ud835\udc5d\ud835\udc5f\ud835\udc5c\ud835\udc63\ud835\udc56\ud835\udc51\ud835\udc52\ud835\udc51,\ud835\udc61\ud835\udc5c\ud835\udc61\ud835\udc4e\ud835\udc59(\u2212\ud835\udc41\ud835\udc52\ud835\udc61\ud835\udc42\ud835\udc37\ud835\udc34\ud835\udc5d\ud835\udc5f\ud835\udc5c\ud835\udc63\ud835\udc56\ud835\udc51\ud835\udc52\ud835\udc51,\ud835\udc61\ud835\udc5c\ud835\udc61\ud835\udc4e\ud835\udc59(\ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5b\ud835\udc612015\ud835\udc48\ud835\udc46) <br/>\n. Net ODA provided, total (current US$)"
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": "# Parameters\n\ncloud = True"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": "# Imports\n\nimport pandas as pd\nimport numpy as np\nimport zipfile\nimport matplotlib.pyplot as plt\nimport zlib\nimport pickle\nfrom threading import Thread\nfrom multiprocessing import Pool\nimport time\nfrom datetime import datetime"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# The code was removed by Watson Studio for sharing."
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": "# Storage instantiation\n\nif cloud:\n    import types\n    import pandas as pd\n    import ibm_boto3\n    from botocore.client import Config\n\n    def __iter__(self): return 0\n\n    cnx_fcsinsights = ibm_boto3.client(\n        service_name = 's3',\n        ibm_api_key_id = parm_ibm_api_key_id,\n        ibm_service_instance_id = parm_ibm_service_instance_id,\n        ibm_auth_endpoint = parm_ibm_auth_endpoint,\n        config = Config(signature_version='oauth'),\n        endpoint_url = parm_endpoint_url\n    )\n\nbkt_in = parm_bkt_in\nbkt_out = parm_bkt_out\n\nkey_in = 'WDI treated-step2.csv.gz'\nkey_out = 'WDI treated-step3.csv.gz'"
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": "# Read data\n\nif cloud:\n    cnx_fcsinsights.download_file(Bucket=bkt_in, Key=key_in, Filename=key_in)\n    df = pd.read_csv(key_in, compression='gzip')\nelse:\n    df = pd.read_csv('F:/WDI Working Files/'+key_in, compression='gzip')"
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": "# Cleansing worker \n\npy = \"\"\"import numpy as np\nimport pandas as pd\nimport zlib\nimport pickle\n\ndef outliers_treatment(params):\n\n    cont = 0\n    part = params.get('part')\n    \n    f = open('thread_'+str(part)+'.txt', mode='w')\n    f.write(str(part)+' '+str(cont))\n    f.close()\n                \n    df_part = pickle.loads(zlib.decompress(params.get('dfz_part')))\n    \n    for country_code in df_part['Country Code'].unique():\n    \n        df_country = df_part[df_part['Country Code']==country_code]\n        \n        for indicator_code in df_country['Indicator Code'].unique():\n\n            df_indicator = df_country[(df_country['Indicator Code']==indicator_code)].copy()\n\n            q1 = df_indicator['Value'].quantile(0.25)\n            q3 = df_indicator['Value'].quantile(0.75)\n            iqr = q3 - q1  \n            lim_inf = q1 - 1.5 * iqr\n            lim_sup = q3 + 1.5 * iqr\n\n            df_indicator['Value'] = df_indicator['Value'].apply(lambda v: lim_inf if v < lim_inf else lim_sup if v > lim_sup else v)\n\n            ixs = df_indicator.index\n            \n            df_part.at[ixs, 'Value'] = df_indicator['Value'].values\n                \n            cont+=1\n            if cont%500==0:\n                f = open('thread_'+str(part)+'.txt', mode='w')\n                f.write(str(part)+' '+str(cont))\n                f.close()\n                \n    dfz_part = zlib.compress(pickle.dumps(df_part))\n    del df_part\n    return dfz_part\"\"\"\n\nf = open('outliers_treatment3.py', mode='w',encoding='utf8')\nf.write(py)\nf.close()\nimport outliers_treatment3"
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": "# Compressing parts\n\ndf = df.reset_index().drop(columns=['index'])\ndfz_part1 = zlib.compress(pickle.dumps(df.query(\"Part==1\")))\ndfz_part2 = zlib.compress(pickle.dumps(df.query(\"Part==2\")))\ndfz_part3 = zlib.compress(pickle.dumps(df.query(\"Part==3\")))\ndfz_part4 = zlib.compress(pickle.dumps(df.query(\"Part==4\")))"
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "acionando thread de processors\n"
                }
            ],
            "source": "# Parallel processing\n\np = Pool(processes = 4)\n\nlst_params = []\nlst_params.append({'part':1, 'dfz_part':dfz_part1})\nlst_params.append({'part':2, 'dfz_part':dfz_part2})\nlst_params.append({'part':3, 'dfz_part':dfz_part3})\nlst_params.append({'part':4, 'dfz_part':dfz_part4})\n\ndef thread_unica(parm, lst_params):\n    \n    print('acionando thread de processors')\n    ret = p.map(outliers_treatment3.outliers_treatment, lst_params)\n    parm.append(ret)\n\nparm = []\nthread = Thread(target=thread_unica, args=(parm, lst_params))\nthread.start()"
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "1 500\n2 500\n3 500\n4 500\n\n1 1000\n2 1000\n3 1000\n4 1000\n\n1 1500\n2 1500\n3 1500\n4 1500\n\n1 2000\n2 2000\n3 2000\n4 2000\n\n1 3000\n2 3000\n3 3000\n4 3000\n\n1 3500\n2 3500\n3 3500\n4 3500\n\n1 4000\n2 4000\n3 4000\n4 4000\n\n1 4500\n2 4500\n3 5000\n4 5000\n\n1 5500\n2 5500\n3 5500\n4 5500\n\n1 6000\n2 6000\n3 6000\n4 6000\n\n1 6500\n2 6500\n3 6500\n4 6500\n\n1 7000\n2 7000\n3 7500\n4 7500\n\n1 8000\n2 8000\n3 8000\n4 8000\n\n1 8500\n2 8500\n3 8500\n4 8500\n\n1 9000\n2 9000\n3 9000\n4 9000\n\n1 10000\n2 10000\n3 10000\n4 10000\n\n1 10500\n2 10500\n3 10500\n4 10500\n\n1 11000\n2 11000\n3 11000\n4 11000\n\n1 11500\n2 11500\n3 12000\n4 12000\n\n1 12500\n2 12500\n3 12500\n4 12500\n\n1 13000\n2 13000\n3 13000\n4 13000\n\n1 13500\n2 13500\n3 13500\n4 13500\n\n1 14500\n2 14000\n3 14500\n4 14500\n\n1 15000\n2 15000\n3 15000\n4 15000\n\n1 15500\n2 15500\n3 15500\n4 15500\n\n1 16500\n2 16000\n3 16000\n4 16000\n\n1 17000\n2 17000\n3 17000\n4 17000\n\n1 17500\n2 17500\n3 17500\n4 17500\n\n1 18000\n2 18000\n3 18000\n4 18000\n\n1 19000\n2 18500\n3 19000\n4 19000\n\n1 19500\n2 19500\n3 19500\n4 19500\n\n1 20000\n2 20000\n3 20000\n4 20000\n\n1 20500\n2 20500\n3 21000\n4 20500\n\n1 21500\n2 21000\n3 21500\n4 21500\n\n1 22000\n2 22000\n3 22000\n4 22000\n\n1 22500\n2 22500\n3 22500\n4 22500\n\n1 23500\n2 23000\n3 23500\n4 23500\n\n1 24000\n2 23500\n3 24000\n4 24000\n\n1 24500\n2 24500\n3 24500\n4 24500\n\n1 25500\n2 25000\n3 25500\n4 25000\n\n1 26000\n2 25500\n3 26000\n4 26000\n\n1 26500\n2 26000\n3 26500\n4 26500\n\n1 27000\n2 27000\n3 27000\n4 27000\n\n1 28000\n2 27500\n3 28000\n4 28000\n\n1 28500\n2 28000\n3 28500\n4 28500\n\n1 29000\n2 28500\n3 29000\n4 29000\n\n1 30000\n2 29500\n3 30000\n4 29500\n\n1 30500\n2 30000\n3 30500\n4 30500\n\n1 31000\n2 30500\n3 31000\n4 31000\n\n1 31500\n2 31000\n3 31500\n4 31500\n\n1 32500\n2 32000\n3 32500\n4 32500\n\n1 33000\n2 32500\n3 33000\n4 33000\n\n1 33500\n2 33000\n3 33500\n4 33500\n\n1 34500\n2 33500\n3 34500\n4 34000\n\n1 35000\n2 34500\n3 35000\n4 35000\n\n1 35500\n2 35000\n3 35500\n4 35500\n\n1 36500\n2 35500\n3 36000\n4 36000\n\n1 37000\n2 36500\n3 37000\n4 36500\n\n1 37500\n2 37000\n3 37500\n4 37500\n\n1 38500\n2 37500\n3 38000\n4 38000\n\n1 39000\n2 38000\n3 39000\n4 38500\n\n1 39500\n2 39000\n3 39500\n4 39000\n\n1 40000\n2 39500\n3 40000\n4 40000\n\n1 41000\n2 40000\n3 41000\n4 40500\n\n1 41500\n2 41000\n3 41500\n4 41000\n\n1 42000\n2 41500\n3 42000\n4 41500\n\n1 43000\n2 42000\n3 42500\n4 42500\n\n1 43500\n2 42500\n3 43500\n4 43000\n\n1 44000\n2 43500\n3 44000\n4 43500\n\n1 44500\n2 44000\n3 44500\n4 44500\n\n1 45500\n2 44500\n3 45500\n4 45000\n\n1 46000\n2 45000\n3 46000\n4 45500\n\n1 46500\n2 46000\n3 46500\n4 46000\n\n1 47500\n2 46500\n3 47500\n4 47000\n\n1 48000\n2 47000\n3 48000\n4 47500\n\n1 48500\n2 48000\n3 48500\n4 48000\n\n1 49500\n2 48500\n3 49000\n4 48500\n\n1 50000\n2 49000\n3 50000\n4 49500\n\n1 50500\n2 49500\n3 50500\n4 50000\n\n1 51500\n2 50500\n3 51000\n4 50500\n\n1 52000\n2 51000\n3 52000\n4 51500\n\n1 52500\n2 51500\n3 52500\n4 52000\n\n1 53000\n2 52500\n3 53000\n4 52500\n\n1 54000\n2 53000\n3 53500\n4 53000\n\n1 54500\n2 53500\n3 54500\n4 54000\n\n1 55000\n2 54000\n3 55000\n4 54500\n\n1 56000\n2 55000\n3 55500\n4 55000\n\n1 56500\n2 55500\n3 56500\n4 55500\n\n1 57000\n2 56000\n3 57000\n4 56500\n\n1 58000\n2 56500\n3 57500\n4 57000\n\n1 58500\n2 57500\n3 58000\n4 57500\n\n1 59000\n2 58000\n3 59000\n4 58500\n\n1 59000\n2 58500\n3 59000\n4 59000\n\n1 59000\n2 59000\n3 59000\n4 59000\n\n1 59000\n2 59000\n3 59000\n4 59000\n"
                }
            ],
            "source": "# Following processing\n\ns1_ant = s2_ant = s3_ant = s4_ant = -1\ns1 = s2 = s3 = s4 = -1\n\nfrozen=False\ntime.sleep(30)\nwhile not frozen:\n    \n    try:\n        f1=open('thread_1.txt',mode='r')\n        s1=f1.read()\n        f1.close()\n    except: pass\n    print(s1)\n    \n    try:\n        f2=open('thread_2.txt',mode='r')\n        s2=f2.read()\n        f2.close()\n    except: pass\n    print(s2)\n    \n    try:\n        f3=open('thread_3.txt',mode='r')\n        s3=f3.read()\n        f3.close()\n    except: pass    \n    print(s3)\n    \n    try:\n        f4=open('thread_4.txt',mode='r')\n        s4=f4.read()\n        f4.close()\n    except: pass    \n    print(s4)   \n    \n    if s1==s1_ant and s2==s2_ant and s3==s3_ant and s4==s4_ant:\n        frozen=True\n        break\n    print()\n    \n    s1_ant = s1\n    s2_ant = s2\n    s3_ant = s3\n    s4_ant = s4\n    \n    time.sleep(30)"
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "0\n1\n2\n3\nok\n"
                }
            ],
            "source": "# Consolidating results\n\nfor i in range(4):\n    print(i)\n    parm[0][i] = pickle.loads(zlib.decompress(parm[0][i]))\n\ndf = pd.concat([parm[0][0], parm[0][1], parm[0][2], parm[0][3]], axis=0)\nprint(\"ok\")"
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": "# Save intermediate result\n\nif cloud:\n    df.to_csv(key_out, compression='gzip', index=False)\n    cnx_fcsinsights.upload_file(Bucket=bkt_out, Key=key_out, Filename=key_out)\nelse:\n    df.to_csv('F:/WDI Working Files/'+key_out, compression='gzip', index=False)"
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "ok\n"
                }
            ],
            "source": "print('ok')"
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.9",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}